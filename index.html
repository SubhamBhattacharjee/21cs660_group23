\<html>
    \<head><title></title>
        <style>
            table, th, td {
      border: 1px solid black;
      padding: 5px;
    }

        </style>
            <link rel="stylesheet" type="text/css" href="https://www.niser.ac.in/~smishra/css/smlab.css">
        <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
    </script>
    </head>
    \<body>
        <header>
            <h1><center>News Articles Clusterings using Word and Doc Embeddings</center></h1>
            <h3><center>Subham Bhattacharjee</center></h3>
        </header>
        <hr />
        <p>
            <h3> Introduction</h3>
        </p>
        <p>
          Document  classification or Document  Categorization is  a  problem  in  library  science, information science and computer science. The task is to assign a document to one or more classes or categories.
          But when the same task is done for the documents without any knowlwdge about the labels or categories, then the problem turns into a clustering problem. Here in this project we are trying to cluster news
          articles using the concept of word embeddings and doc embeddings.
        </p>
        <p><h3>What are Word Embeddings:</h3></p>
        <p>Word Embeddings are a type of learned word representation that allows words with similar meaning to have a similar representation. 
          Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. The idea will be more clear by the following example. <br>
          The following figure shows vector representation of some words in a two dimensional vector space.
        </p>
        <div class="square">
          <div> 
              <center><img src= 
                "word embeddings.png" width="350" height="350"
                                    ></center> 
          </div>
          <p></p>
        <p>
          <h3>Idea and Pathway</h3>
        </p>
        <p>
          The project is divided into the following three stages.  
          <ul>
            <li><b><h5>Preprocessing of Articles:</h5></b>This stage is all about cleaning and organizing of the news articles. This includes first the tokenizing of the words of each article. After this comes the stages 
            turning all the words into lower cases so that all same words treated differently are not treated as different words. There are certain words which are regularly used in all types of documents like <b>articles</b>
            ,<b>prepositions</b> etc. Such words in general does not tell anything significant about what the article is saying. Such words are called <b>stopwords</b>. There are also certain cases where combination of 
            two or more words tell us about a significatnt topic but individually they may tell adifferent story. For example <b>New York</b> together means a city but separately as <b>New and York</b> they are very general,
            like <b>New</b> is general word. Other examples are <b>Queen Elizabeth</b> etc. Such words also have to be taken care of. 
          </li>
            <li><b><h5>Generation of Vectors:</h5></b>After the preprocessing of the atrticles in the previous stage this stage will be the vector genaration for the news articles. Now for the generation of vectors of each 
              words there are different embedding techniques available like <b>Word2Vec<a href="index_Subham.html#MikolovWord2Vec">(1)</a></b>, <b>BERT</b>, <b>Fasttext</b>. They are already implemented and can be used directly.  
              But for creating vectors for a doccument Mikolov<a href="index_Subham.html#MikolovWord2Vec">(3)</a></li> gave the idea of how to give a efficient representation of documents or sentences (Doc Embeddings). So here
              those cocepts will be used for the creation of the vectors for the articles. 
            <li><b><h5>Clustering and Analysis:</h5></b>This is the stage where the vectors gained in the previous stage will be applied on the clustering algorithms. Here I am planning to to use various classical
            clustering algorithms like K-Means, DBSCAN etc. After the clustering is done we can bring out the frequent words occuring in a particular cluster to predict what type of news articles are present in that 
          particular cluster say sports news, business news, political news etc. This stage is not a single run stage, its a cyclic stage. It means that if we do not get proper words from the clusters then we can apply 
          other clustering algorithms or perform some improvements again in the text preprocessing to get better results. For the clustering I am referring to the paper<a href="index_Subham.html#MikolovWord2Vec">(2)</a>.
          It is a review paper of 2017 which dicusses about the various clusterinf algorithms./li>
          </ul>
        </p>
        <p>
          <h3>Dataset</h3>
        </p>
        <p>There are two datasets which I have choosen for the project</p>
        <ol>
          <li><a href="https://www.kaggle.com/snapcrack/all-the-news">All the news</a></li>
          <li><a href="https://data.world/opensnippets/al-jazeera-news-dataset">Al Jazeera English News</a></li>
        </ol>
        <p>
          Both these datasets are news datasets. Though datasets contain many field like TITLE, CONTENT, ID , AUTHOR , DOP etc but I will mainly focus on the fields from which we get relevant information. 
        </p>
        <p>
          <h3>What to expect by Midway and post midway</h3>
        </p>
        <p>By midway I expect to complete the data preprocessing and generation of vectors of the articles. If possible I will try to apply the clustering algorithms to genarate some results. In post midway phase 
          I will try to apply more clustering algorithms and hope to apply some neural networks techniques and try to improve the techniques in the preprocessing phase if possible and try to get better results.
        </p>
        <p>
          <h3>Preprocessing of the Articles</h3>
        </p>
        <p>
          Here I have picked the content of the news as the primary source of information about the news. I have decided to consider the title/heading of the news later because the
          title contains some of the important words which can be useful during clustering. After collection of articles the preprocessing started with separation of individual
          words in the article followed by conversion of each words lower case. During preprocessing of the articles I have removed stopwords, for this purpose I have used nltk 
          standard stopwords package. Here is a snapshot of what the news articles look like after the preprocessing.
        </p>
        <div> 
          <center><img src= 
            "preprocessing.jpg" width="1600" height="400"
                                ></center> 
        </div>
        <p>
          In this preprocessing there are few problems like dates are being treated as individual words. For example <b>11 September 2001</b> is an important date but and is important
          conveying the topic of the news but here it is being treated as three different words 11, september and 2001. Similar are the problems with word combos like '<b>1 billion</b>'
          being treated separately. So these are the things which are to be taken care of. Here I also have not used any stemming and lemmatizing, but I plan on using them later to 
          if it helpls in any betterment.
        </p>
          <p>
            <h3>Creation of embeddings for the doccuments</h3>
          </p>
          <p>
            For the creation of the embeddings for the news articles I have used the <b>gendim:doc2vec package</b>. Doc2Vec is based on word2vec but instead of generating
            embeddings for words it generates embeddings for variable length doccuments.  Now words maintain logical (grammatical) structure but documents donâ€™t have any logical
             structures. To solve this problem another vector (Paragraph ID) needs to add with word2vec model. This is the only difference between word2vec and doc2vec. One thing to
             be noted is <b>ParagraphID</b> is a unique document ID.<br>
             Now there are two versions of doc2vec available
          </p>
          <ul>
            <li>Distributed Memory Model of Paragraph Vectors (PV-DM)(Similar to <b>Continuous Bag of Words</b> model of Wrod2Vec)</li>
            <li>Distributed Bag of Words version of Paragraph Vector (PV-DBOW)(Similar to <b>Skip-gram</b> model of Wrod2Vec)</li>
          </ul>
          <p>We will have a little idea about both of them.</p>
          <p><h4>Distributed Memory Model</h4></p>
          <p>Distributed Memory (DM) model is similar to Continuous-Bag-of-Words (CBOW) model in word2vec which attempts to guess the output (target word) from its neighboring words 
            (context words) with the addition of a paragraph ID. Lets say we have a single doccument say <p style="color:blue;">"I like natural language processing"</p> and it will
          be predicting next word for a given word. Then the model will look like below</p>
          <div> 
            <center><img src= 
              "DM model.png" width="400" height="400"
                                  ></center> 
          </div>
          <p>So here it learns to predict a word based on the words present in the context. Here it trains the doccument vector along with the words with the intution that given 
            the vector of the doccument, it should be good enough to predict the words in the document.
          </p>


          <p><h4>Distributed Bag of Words</h4></p>
          <p>Distributed Bag-Of-Words (DBOW) Model similar to skip-gram model of word2vec, which guesses the context words from a target word. The following figure explains it</p>
          <div> 
            <center><img src= 
              "DBOW.png" width="400" height="400"
                                  ></center> 
          </div>
          <p>So here it learns to predict the context words based on the doccument. There is only one difference between skip-gram and distributed bag of words (DBOW) is instead of 
            using the target word as the input, Distributed Bag of Words (DBOW) takes the document ID (Paragraph ID) as the input and tries to predict randomly sampled words from the
             document.
          </p>
          <p><h3>Applying the Clustering Algorithms</h3></p>
          <p>After the creation of the doc vectors for the articles I applied the K-means clustering algorithms. Here I have choosen vector length of 2 for simplicity in visualization.
            After the vectors of the documents were created and when we plotted it 2D it looked like the following.
          </p>
          <div> 
            <center><img src= 
              "aijazeer.png" width="600" height="400"
                                  ></center> 
          </div>
          <p>So this plot looks like all the news are in the single cluster at first glance but since this cluster is spread out horizontally so news articles present at the extreme
            ends of the clusters have high chance to be of different topics because their vector representations are quite separated from each other. So my guess before the clutering 
            was it would be best to have two clusters out of this plot. So to confirm this elbow method was conducted. Here we calculated the inertia score for cluster numbers 1 to 10.
            <b>Inertia</b> is the sum of squared distances of samples to their closest cluster centre. It is also sometimes called <b>Sum of Squared Errors(SSE).</b> The following is the expression for inertia.
          </p>
         <center>$$ SSE= \sum_{i=1}^n\sum_{j=1}^k w^{(i,j)}|x^i-\mu^j|_2^2$$<br>
        Here $$\mu^j$$ is the center for cluster j<br>
      and $$   w^{(i,j)}=1\ \text{if the sample } x^i \text{is in cluster} j,\ 
      0\ \text{otherwise} $$</center> 
      <p>The output of the elbow method looked like the following</p>

 <div> 
            <center><img src= 
              "aijazeera inertia.png" width="600" height="400"
                                  ></center> 
          </div>

<p>The elbow method confirmed that 2 clusters is optimal. So after applying the K-means algorithm for two clusters the output was the following</p>
<div> 
  <center><img src= 
    "aijazeera 2cls.png" width="600" height="400"
                        ></center> 
        <p><h3>Future Planning</h3></p>
        <p>This was all that has been done before the midway presentations. The next target are to improve the preprocessing techniques to capture the problems discussd in the 
          preprocessing stage. Further after till now only the content of the news were used to create the doccuments, so the next target will be to include the text in the title 
          and the headings of the news. Finally after this next comes the topic modelling which tells about the overall ideas the news of any particular cluster is speaking about.
        </p>
</div>

        <ol>
          <li>
             <p><a name="MikolovWord2Vec"><cite>Mikolov, Tomas & Chen, Kai & Corrado, G.s & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. Proceedings of Workshop at ICLR. 2013.</cite></a></p>
          </li>
          <li>
             <p><cite><a name="Clustering Review">Saxena, Amit & Prasad, Mukesh & Gupta, Akshansh & Bharill, Neha & Patel, op & Tiwari, Aruna & Er, Meng & Lin, Chin-Teng. (2017). A Review of Clustering Techniques and Developments. 
               Neurocomputing. 267. 10.1016/j.neucom.2017.06.053.</a> </cite></p>
          </li>
          <li>
            <p><cite><a name="MikolovDoc2Vec">Le, Quoc & Mikolov, Tomas. (2014). Distributed Representations of Sentences and Documents. 31st International Conference on Machine Learning, ICML 2014. 4.</a> </cite></p>
         </li>
       </ol>
    
    </body>
</html>
